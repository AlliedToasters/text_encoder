{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, activations\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "def byte_me(input_string):\n",
    "    \"\"\"Converts the input string to an array of\n",
    "    integers.\"\"\"\n",
    "    sl = 32 #sequence length\n",
    "    b = bytearray()\n",
    "    b.extend(input_string.encode())\n",
    "    output = np.zeros(sl, dtype=np.uint8)\n",
    "    result = np.array(b)[:sl]\n",
    "    x = min(len(result), sl)\n",
    "    output[:x] = result\n",
    "    return output.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 64)     16384       input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 256), (None, 328704      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 256),  328704      embedding_5[1][0]                \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, None, 256)    65792       lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 739,584\n",
      "Trainable params: 739,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_seq2seq_lstm(num_encoder_tokens, latent_dim, num_decoder_tokens=None, emb_dim=64):\n",
    "    inp_char_embedder = Embedding(num_encoder_tokens, emb_dim)\n",
    "    \n",
    "    if num_decoder_tokens is not None:\n",
    "        outp_char_embedder = Embedding(num_decoder_tokens, emb_dim)\n",
    "    else:\n",
    "        outp_char_embedder = inp_char_embedder\n",
    "        num_decoder_tokens = num_encoder_tokens\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    embedded_encoder_inputs = inp_char_embedder(encoder_inputs)\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(embedded_encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    embedded_decoder_inputs = outp_char_embedder(decoder_inputs)\n",
    "\n",
    "\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(embedded_decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model, encoder_states\n",
    "\n",
    "num_encoder_tokens = 256\n",
    "latent_dim = 256\n",
    "\n",
    "autoencoder, states = build_seq2seq_lstm(num_encoder_tokens, latent_dim)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00387643, 0.00391192, 0.00389487, ..., 0.00389812,\n",
       "         0.00390685, 0.00391295],\n",
       "        [0.00387639, 0.00389949, 0.00390452, ..., 0.00389897,\n",
       "         0.00390226, 0.00390655],\n",
       "        [0.00387394, 0.00390868, 0.00390311, ..., 0.00390289,\n",
       "         0.00390836, 0.00391922],\n",
       "        ...,\n",
       "        [0.00391605, 0.00390067, 0.003907  , ..., 0.00390029,\n",
       "         0.00390275, 0.00391897],\n",
       "        [0.00390654, 0.00388774, 0.00390362, ..., 0.00390388,\n",
       "         0.00390672, 0.00391707],\n",
       "        [0.00391581, 0.0038845 , 0.00390939, ..., 0.0039181 ,\n",
       "         0.00388666, 0.00392168]],\n",
       "\n",
       "       [[0.00387643, 0.00391192, 0.00389487, ..., 0.00389812,\n",
       "         0.00390685, 0.00391295],\n",
       "        [0.00387639, 0.00389949, 0.00390452, ..., 0.00389897,\n",
       "         0.00390226, 0.00390655],\n",
       "        [0.00387394, 0.00390868, 0.00390311, ..., 0.00390289,\n",
       "         0.00390836, 0.00391922],\n",
       "        ...,\n",
       "        [0.00391605, 0.00390067, 0.003907  , ..., 0.00390029,\n",
       "         0.00390275, 0.00391897],\n",
       "        [0.00390654, 0.00388774, 0.00390362, ..., 0.00390388,\n",
       "         0.00390672, 0.00391707],\n",
       "        [0.00391581, 0.0038845 , 0.00390939, ..., 0.0039181 ,\n",
       "         0.00388666, 0.00392168]]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '„Åça completely different example from the one I had been using before this allwent to shit'\n",
    "out = byte_me(string)\n",
    "\n",
    "out = np.concatenate([out, out])\n",
    "autoencoder.predict([out, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('many_queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')\n",
    "df['lns'] = df['query'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.132663e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.034532e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.876606e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>3.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.418000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lns\n",
       "count  2.132663e+07\n",
       "mean   1.034532e+01\n",
       "std    6.876606e+00\n",
       "min    0.000000e+00\n",
       "50%    9.000000e+00\n",
       "99%    3.200000e+01\n",
       "max    1.418000e+03"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['lns']].describe(percentiles=[.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = df['query'].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = vals[:100000]\n",
    "rows = [byte_me(x) for x in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate(rows)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(X, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "charbank = 'abcdefghijklmnopqrstuvwxyz'\n",
    "charbank = charbank + charbank.upper()\n",
    "charbank = charbank + '0123456789'\n",
    "\n",
    "def deletion(string):\n",
    "    \"\"\"Performs a random deletion of input string\"\"\"\n",
    "    to_del = np.random.randint(0, high=len(string))\n",
    "    return string[:to_del] + string[to_del+1:]\n",
    "\n",
    "def insertion(string, charbank=charbank):\n",
    "    \"\"\"Performs a random insertion into input string\"\"\"\n",
    "    to_ins = np.random.randint(0, high=len(string))\n",
    "    char = np.random.choice(list(charbank))\n",
    "    return string[:to_ins] + char + string[to_ins:]\n",
    "\n",
    "def swap(string):\n",
    "    \"\"\"swaps two consecutive characters in string.\"\"\"\n",
    "    to_swap = np.random.randint(0, high=(len(string)-1))\n",
    "    return string[:to_swap] + string[to_swap+1] + string[to_swap] + string[to_swap+2:]\n",
    "\n",
    "def apply_noise(string):\n",
    "    \"\"\"Randomly applies one type of noise.\"\"\"\n",
    "    if len(string) < 2:\n",
    "        return string\n",
    "    func = np.random.choice([deletion, insertion, swap])\n",
    "    return func(string)\n",
    "\n",
    "def random_gen(batch_size=32):\n",
    "    \"\"\"\n",
    "    For training the identity function.\n",
    "    Generates random sequences.\n",
    "    \"\"\"\n",
    "    sl = 32 #sequence length\n",
    "    while True:\n",
    "        X_out = np.random.randint(0, high=256, size=(batch_size, sl))\n",
    "        Y_out = np.array([to_categorical(x, num_classes=256) for x in X_out])\n",
    "        yield X_out, Y_out\n",
    "\n",
    "\n",
    "def data_gen(X, batch_size=32):\n",
    "    \"\"\"\n",
    "    For training the identity function on real queries.\n",
    "    Generates identity samples of queries.\n",
    "    \"\"\"\n",
    "    sl = 32 #sequence length\n",
    "    while True:\n",
    "        idx = np.random.randint(len(X), size=(batch_size))\n",
    "        X_out = X[idx]\n",
    "        X_dec = np.zeros(shape=X_out.shape, dtype=X_out.dtype)\n",
    "        X_dec[:, 1:] = X_out[:, :-1]\n",
    "        Y_out = np.array([to_categorical(x, num_classes=256) for x in X_out])\n",
    "        yield [X_out, X_dec], Y_out\n",
    "        \n",
    "def noise_gen(X, batch_size=32):\n",
    "    \"\"\"\n",
    "    For training the identity function on real queries.\n",
    "    Generates identity samples of queries.\n",
    "    \"\"\"\n",
    "    sl = 32 #sequence length\n",
    "    while True:\n",
    "        idx = np.random.randint(len(X), size=(batch_size))\n",
    "        X_out = X[idx]\n",
    "        Y_out = np.array([to_categorical(x, num_classes=256) for x in X_out])\n",
    "        X_ = []\n",
    "        for i, x in enumerate(X_out):\n",
    "            try:\n",
    "                x = bytearray(x).split(b'\\0',1)[0].decode()\n",
    "            except UnicodeDecodeError:\n",
    "                #remove examples where bad unicode.\n",
    "                Y_out = np.concatenate([Y_out[:i], Y_out[i+1:]], axis=0)\n",
    "                continue\n",
    "            x = apply_noise(x)\n",
    "            x = byte_me(x)\n",
    "            X_.append(x)\n",
    "        X_out = np.concatenate(X_)\n",
    "        #todo: debug cases where len(x) != len(y)\n",
    "        if len(X_out) == len(Y_out):\n",
    "            yield X_out, Y_out\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "trg = data_gen(train)\n",
    "teg = data_gen(val)\n",
    "gen = random_gen()\n",
    "ntrg = noise_gen(train)\n",
    "nteg = noise_gen(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74, 111, 114, 100,  97, 110,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "333/512 [==================>...........] - ETA: 28s - loss: 0.8482 - acc: 0.7906"
     ]
    }
   ],
   "source": [
    "#identity step\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "autoencoder.compile(\n",
    "    optimizer=optimizers.Adam(lr=.001), \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "autoencoder.fit_generator(\n",
    "    generator=trg,\n",
    "    validation_data=teg,\n",
    "    steps_per_epoch=512,\n",
    "    validation_steps=100,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116 101 115 116  32 116 104 105 115  32 115 116 114 105 110 103   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[ 26  13 210 217  32  88 220 139 145 225 112 180 114  86 186 220   0   0\n",
      "    7   0 243  15   0  97   0  31  31  31   0   7 169 254]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\x1a\\r'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(model, example='testthisthang'):\n",
    "    example = example\n",
    "    inp = byte_me(example)\n",
    "    print(inp)\n",
    "    out = model.predict(inp)\n",
    "    out = np.argmax(out, axis=2).astype(np.uint8)\n",
    "    print(out)\n",
    "    return encode_output(out)\n",
    "\n",
    "def encode_output(array):\n",
    "    \"\"\"\n",
    "    encodes neural network output to unicode.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return bytearray(array).split(b'\\0',1)[0].decode()\n",
    "    except UnicodeDecodeError:\n",
    "        i = 1\n",
    "        while True:\n",
    "            try:\n",
    "                return bytearray(array).split(b'\\0',1)[0][:-i].decode()\n",
    "            except UnicodeDecodeError:\n",
    "                i+=1\n",
    "                \n",
    "test_model(autoencoder, 'test this string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5827\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.6295 - val_loss: 0.5827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe72c4246d8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no noise\n",
    "\n",
    "autoencoder.fit_generator(\n",
    "    generator=trg,\n",
    "    validation_data=teg,\n",
    "    steps_per_epoch=4096,\n",
    "    validation_steps=42,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116 101 115 116  32 116 104 105 115  32 115 116 114 105 110 103  32 111\n",
      "  117 116  32 115 101 101  32 119 104  97 116  32 121 111]]\n",
      "[[ 69 101  42 118  32 184  67 239 115  32 115 180 114 105 110 103  55 111\n",
      "  117 116  32 107  97 139  48 158 104  97  66  32  29 111]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ee*v '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(autoencoder, 'test this string out see what you get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 1.1356\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 1.3720 - val_loss: 1.1356\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.9698\n",
      "4096/4096 [==============================] - 249s 61ms/step - loss: 1.0325 - val_loss: 0.9698\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.9074\n",
      "4096/4096 [==============================] - 251s 61ms/step - loss: 0.9258 - val_loss: 0.9074\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.7411\n",
      "4096/4096 [==============================] - 241s 59ms/step - loss: 0.8352 - val_loss: 0.7411\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6903\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.7017 - val_loss: 0.6903\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6582\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.6777 - val_loss: 0.6582\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.6652\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.6547 - val_loss: 0.6652\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6359\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.6407 - val_loss: 0.6359\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6378\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.6305 - val_loss: 0.6378\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5797\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.6203 - val_loss: 0.5797\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5990\n",
      "4096/4096 [==============================] - 225s 55ms/step - loss: 0.6075 - val_loss: 0.5990\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5989\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5989 - val_loss: 0.5989\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5834\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5878 - val_loss: 0.5834\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5856\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5850 - val_loss: 0.5856\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5906\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5770 - val_loss: 0.5906\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5683\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5720 - val_loss: 0.5683\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5851\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5700 - val_loss: 0.5851\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5770\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5667 - val_loss: 0.5770\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5738\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5611 - val_loss: 0.5738\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5618\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5571 - val_loss: 0.5618\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5441\n",
      "4096/4096 [==============================] - 226s 55ms/step - loss: 0.5581 - val_loss: 0.5441\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5833\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5548 - val_loss: 0.5833\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5712\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5539 - val_loss: 0.5712\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5732\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5505 - val_loss: 0.5732\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5282\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5481 - val_loss: 0.5282\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5353\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5447 - val_loss: 0.5353\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5597\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5451 - val_loss: 0.5597\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5664\n",
      "4096/4096 [==============================] - 227s 55ms/step - loss: 0.5428 - val_loss: 0.5664\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5425\n",
      "4096/4096 [==============================] - 228s 56ms/step - loss: 0.5412 - val_loss: 0.5425\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5454\n",
      "4096/4096 [==============================] - 228s 56ms/step - loss: 0.5407 - val_loss: 0.5454\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5270\n",
      "4096/4096 [==============================] - 228s 56ms/step - loss: 0.5416 - val_loss: 0.5270\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5607\n",
      "4096/4096 [==============================] - 228s 56ms/step - loss: 0.5417 - val_loss: 0.5607\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5775\n",
      "4096/4096 [==============================] - 228s 56ms/step - loss: 0.5389 - val_loss: 0.5775\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5467\n",
      "4096/4096 [==============================] - 229s 56ms/step - loss: 0.5381 - val_loss: 0.5467\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5358\n",
      "4096/4096 [==============================] - 229s 56ms/step - loss: 0.5365 - val_loss: 0.5358\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5456\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5376 - val_loss: 0.5456\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5544\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5361 - val_loss: 0.5544\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5358\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5351 - val_loss: 0.5358\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5367\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5344 - val_loss: 0.5367\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5364\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5323 - val_loss: 0.5364\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5263\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5332 - val_loss: 0.5263\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5302\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5331 - val_loss: 0.5302\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5488\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5311 - val_loss: 0.5488\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5369\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5309 - val_loss: 0.5369\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5305\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5287 - val_loss: 0.5305\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5344\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5305 - val_loss: 0.5344\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5298\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5303 - val_loss: 0.5298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5362\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5299 - val_loss: 0.5362\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5231\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5286 - val_loss: 0.5231\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5368\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5296 - val_loss: 0.5368\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5470\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5276 - val_loss: 0.5470\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5346\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5273 - val_loss: 0.5346\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5287\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5277 - val_loss: 0.5287\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5385\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5274 - val_loss: 0.5385\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5498\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5276 - val_loss: 0.5498\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5341\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5236 - val_loss: 0.5341\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5192\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5250 - val_loss: 0.5192\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5387\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5238 - val_loss: 0.5387\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5264\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5248 - val_loss: 0.5264\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5206\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5251 - val_loss: 0.5206\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5294\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5238 - val_loss: 0.5294\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5218\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5222 - val_loss: 0.5218\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5416\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5211 - val_loss: 0.5416\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5291\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5239 - val_loss: 0.5291\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5537\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5231 - val_loss: 0.5537\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5157\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5247 - val_loss: 0.5157\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5143\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5241 - val_loss: 0.5143\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5404\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5231 - val_loss: 0.5404\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5004\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5218 - val_loss: 0.5004\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5295\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5206 - val_loss: 0.5295\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5238\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5214 - val_loss: 0.5238\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5133\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5190 - val_loss: 0.5133\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5561\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5226 - val_loss: 0.5561\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5190\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5210 - val_loss: 0.5190\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5322\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5212 - val_loss: 0.5322\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5269\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5202 - val_loss: 0.5269\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5005\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5195 - val_loss: 0.5005\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5402\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5225 - val_loss: 0.5402\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5204\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5203 - val_loss: 0.5204\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5156\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5213 - val_loss: 0.5156\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5449\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5176 - val_loss: 0.5449\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5112\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5194 - val_loss: 0.5112\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5414\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5192 - val_loss: 0.5414\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5220\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5198 - val_loss: 0.5220\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5187\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5205 - val_loss: 0.5187\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5282\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5198 - val_loss: 0.5282\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5311\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5193 - val_loss: 0.5311\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5116\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5198 - val_loss: 0.5116\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5232\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5181 - val_loss: 0.5232\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5309\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5181 - val_loss: 0.5309\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5336\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.5182 - val_loss: 0.5336\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4908\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.5134 - val_loss: 0.4908\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4175\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.4949 - val_loss: 0.4175\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.3989\n",
      "4096/4096 [==============================] - 223s 55ms/step - loss: 0.4012 - val_loss: 0.3989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.3969\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.3950 - val_loss: 0.3969\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.3913\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.3944 - val_loss: 0.3913\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.3953\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.3946 - val_loss: 0.3953\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.3958\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.3944 - val_loss: 0.3958\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.3919\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.3925 - val_loss: 0.3919\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4044\n",
      "4096/4096 [==============================] - 224s 55ms/step - loss: 0.3915 - val_loss: 0.4044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe72c4242e8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noise\n",
    "\n",
    "autoencoder.fit_generator(\n",
    "    generator=ntrg,\n",
    "    validation_data=nteg,\n",
    "    steps_per_epoch=4096,\n",
    "    validation_steps=42,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 0, 1, 0, 2, 2, 2, 1, 1, 0, 0, 0, 2, 2, 0, 1, 2, 1, 1, 1, 1,\n",
       "       0, 1, 0, 2, 0, 2, 0, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 1, 2, 2, 1, 2,\n",
       "       0, 0, 1, 0, 2, 2, 2, 1, 0, 2, 1, 1, 2, 2, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 2, 2, 0, 1, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(3, size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116 101 115 116  32 116 104 105 115  32 115 116 114 105 110 103  32 111\n",
      "  117 116  32 115 101 101  32 119 104  97 116  32 121 111]]\n",
      "[[ 84   1  97 116 116  32 105 105 112 115 112 115 105 105 110 103 103 116\n",
      "  114 116  32  32 104 104 114  32  32  97  97 101   0 101]]\n",
      "T\u0001att iipspsiinggtrt  hhr  aae\n",
      "[[105  97 114 109  97 120  32  51  54  53   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[ 97 105 114 109  97   1  32  51  53  53   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "airma\u0001 355\n",
      "[[103 105 118 101  32 109 101  32 107 114 105 101   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "[[103 105   1  98  32 109 101  32 101 114 101 101   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "gi\u0001b me eree\n"
     ]
    }
   ],
   "source": [
    "print(test_model(autoencoder, 'test this string out see what you get'))\n",
    "print(test_model(autoencoder, 'iarmax 365'))\n",
    "print(test_model(autoencoder, 'give me krie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
